# sparse embedding model - simple version for inference only
from typing import List, Dict, Any
import math
from collections import defaultdict
import sys
import os
import argparse
import importlib.util
import json
from pathlib import Path
from kiwipiepy import Kiwi
from pypdf import PdfReader
from dotenv import load_dotenv

ENV_PATH = Path(__file__).resolve().parents[3] / ".env"
load_dotenv(ENV_PATH)

ROOT_DIR = Path(__file__).resolve().parents[3]
if str(ROOT_DIR) not in sys.path:
    sys.path.insert(0, str(ROOT_DIR))


# Initialize Kiwi tokenizer
try:
    kiwi = Kiwi()
    KIWI_AVAILABLE = True
    # print("✓ Kiwipiepy loaded successfully")
except ImportError:
    KIWI_AVAILABLE = False
    # print("⚠ Kiwipiepy not available. Install: pip install kiwipiepy")


def tokenize_korean(text: str) -> List[str]:
    """
    Tokenize Korean text using Kiwi morphological analyzer
    Falls back to simple split if Kiwi is not available

    Args:
        text: Input text

    Returns:
        List of tokens (nouns and base forms)
    """
    if not KIWI_AVAILABLE:
        # Fallback to simple tokenization
        return text.lower().split()

    # Use Kiwi to extract morphemes
    result = kiwi.tokenize(text)
    tokens = []

    for token in result:
        # Each token has .form (surface form) and .tag (POS tag)
        pos = token.tag
        form = token.form.lower()

        # Include nouns and important POS tags
        if pos in ["NNG", "NNP", "NNB"]:  # Common noun, Proper noun, Bound noun
            tokens.append(form)
        elif pos in ["VV", "VA"]:  # Verb, Adjective (base forms)
            tokens.append(form)

    return tokens


def get_terms_dir() -> Path:
    try:
        from app.agents.document_parser.constants import TERMS_DIR

        return Path(TERMS_DIR)
    except Exception:
        return (
            Path(__file__).resolve().parents[1] / "document_parser" / "data" / "terms"
        )


def load_chunks_from_document_parser(source_doc_name: str) -> List[Dict[str, Any]]:
    """
    Load tagged chunk files generated by document_parser.

    Expected structure:
    app/agents/document_parser/data/terms/<source_doc_stem>/chunks/*.py
    """
    source_doc_stem = Path(source_doc_name).stem

    terms_dir = get_terms_dir()

    chunks_dir = terms_dir / source_doc_stem / "chunks"
    if not chunks_dir.exists():
        print(f"[WARN] document_parser chunks directory not found: {chunks_dir}")
        return []

    chunk_files = sorted(chunks_dir.glob(f"{source_doc_stem}_*.py"))
    chunks: List[Dict[str, Any]] = []

    for idx, file_path in enumerate(chunk_files):
        if file_path.name.endswith("_tagging_summary.py"):
            continue

        try:
            module_name = f"dp_chunk_{source_doc_stem}_{idx}"
            spec = importlib.util.spec_from_file_location(module_name, str(file_path))
            if spec is None or spec.loader is None:
                continue

            module = importlib.util.module_from_spec(spec)
            spec.loader.exec_module(module)
            chunk_obj = getattr(module, "chunk", None)
            if not isinstance(chunk_obj, dict):
                continue

            metadata = chunk_obj.get("metadata", {})
            doc_meta = metadata.get("doc", {}) if isinstance(metadata, dict) else {}
            indexing_meta = (
                metadata.get("indexing", {}) if isinstance(metadata, dict) else {}
            )

            chunk_id = indexing_meta.get("chunk_id") or file_path.stem
            page = doc_meta.get("page")
            text = chunk_obj.get("page_content", "")

            chunks.append(
                {
                    "id": chunk_id,
                    "page": page,
                    "text": text,
                    "metadata": metadata,
                }
            )
        except Exception:
            continue

    return chunks


def load_chunks_from_terms_pdf(source_doc_name: str) -> List[Dict[str, Any]]:
    """
    Load PDF directly from document_parser terms dir and create page-level chunks.
    """
    terms_dir = get_terms_dir()
    pdf_path = terms_dir / source_doc_name
    if not pdf_path.exists():
        print(f"[WARN] source PDF not found in terms dir: {pdf_path}")
        return []

    try:
        reader = PdfReader(str(pdf_path))
    except Exception as exc:
        print(f"[WARN] failed to read PDF: {pdf_path} ({exc})")
        return []

    chunks: List[Dict[str, Any]] = []
    for idx, page in enumerate(reader.pages, start=1):
        try:
            text = page.extract_text() or ""
        except Exception:
            text = ""
        if not text.strip():
            continue
        chunks.append(
            {
                "id": f"p{idx}",
                "page": idx,
                "text": text,
                "metadata": {"doc": {"file_name": source_doc_name, "page": idx}},
            }
        )

    return chunks


def save_vocab_json(
    output_dir: Path,
    vocab: Dict[str, int],
    idf: Dict[str, float],
    predefined_words: List[str],
) -> None:
    output_dir.mkdir(parents=True, exist_ok=True)
    output_path = output_dir / "vocab.json"
    payload = {
        "vocab": vocab,
        "idf": idf,
        "predefined_words": predefined_words,
    }
    with output_path.open("w", encoding="utf-8") as fp:
        json.dump(payload, fp, ensure_ascii=True)
    print(f"Saved vocab to: {output_path}")


# build TF-IDF model from corpus
def build_tfidf(chunks: List[Dict[str, Any]], predefined_words: List[str] = None):
    vocab: Dict[str, int] = {}
    df = defaultdict(int)
    docs_tokens: List[List[str]] = []

    # collect tokens and document frequencies from corpus
    for c in chunks:
        text_lower = c["text"].lower()
        # Use Korean tokenizer instead of simple split
        tokens = tokenize_korean(c["text"])

        # Add predefined words if found as substrings in chunk text
        if predefined_words:
            for pw in predefined_words:
                if pw.lower() in text_lower:
                    tokens.append(pw.lower())

        docs_tokens.append(tokens)
        for t in set(tokens):
            df[t] += 1
            if t not in vocab:
                vocab[t] = len(vocab)

    # ensure predefined words are present in vocab/df (df default 0)
    if predefined_words:
        for pw in predefined_words:
            pw_lower = pw.lower()
            if pw_lower not in vocab:
                vocab[pw_lower] = len(vocab)
            # ensure df key exists (leave count as-is if present)
            if pw_lower not in df:
                df[pw_lower] = 0

    N = len(chunks)
    idf = {t: math.log(N / (1 + df[t])) for t in df}

    sparse_vectors: Dict[str, List[tuple]] = {}
    for i, tokens in enumerate(docs_tokens):
        tf = defaultdict(int)
        for t in tokens:
            tf[t] += 1
        vec = [(vocab[t], tf[t] * idf[t]) for t in tf if t in vocab]
        sparse_vectors[chunks[i]["id"]] = vec

    return sparse_vectors, vocab, idf


# convert query words to sparse vector
def query_to_sparse(
    words: List[str], vocab: Dict[str, int], idf: Dict[str, float]
) -> List[tuple]:
    # words are predefined words that matched the query (substring-based)
    tf = defaultdict(int)
    for word in words:
        word_lower = word.lower()
        if word_lower in vocab:
            tf[word_lower] += 1

    query_vec = []
    for token, count in tf.items():
        idx = vocab[token]
        idf_score = idf.get(token, 0.0)
        tfidf_score = count * idf_score
        query_vec.append((idx, tfidf_score))

    return query_vec


# cosine similarity between two sparse vectors
def cosine_similarity(vec1: List[tuple], vec2: List[tuple]) -> float:
    d1 = {idx: val for idx, val in vec1}
    d2 = {idx: val for idx, val in vec2}
    dot = sum(d1.get(idx, 0) * val for idx, val in vec2)
    norm1 = math.sqrt(sum(v**2 for v in d1.values()))
    norm2 = math.sqrt(sum(v**2 for v in d2.values()))
    if norm1 == 0 or norm2 == 0:
        return 0.0
    return dot / (norm1 * norm2)


# search top-3 chunks
def search_top3(
    query_vec: List[tuple],
    sparse_vectors: Dict[str, List[tuple]],
    chunks: List[Dict[str, Any]],
) -> List[tuple]:
    scores = []
    for chunk in chunks:
        chunk_vec = sparse_vectors.get(chunk["id"], [])
        sim = cosine_similarity(query_vec, chunk_vec)
        scores.append((chunk["id"], sim, chunk.get("page"), chunk.get("text")[:100]))
    return sorted(scores, key=lambda x: -x[1])[:3]


# TF-IDF based scoring weights
def calculate_tfidf_weights(
    matched_query_words: List[str],
    idf: Dict[str, float],
) -> Dict[str, float]:
    """
    Calculate TF-IDF-like query weights for matched words.
    Weight(token) = TF_in_query(token) * IDF(token)

    Args:
        matched_query_words: List of matched predefined words
        idf: IDF dictionary

    Returns:
        Dictionary of word -> TF-IDF-like weight mapping
    """
    tf = defaultdict(int)
    for word in matched_query_words:
        word_lower = word.lower()
        tf[word_lower] += 1

    weights: Dict[str, float] = {}
    for token, count in tf.items():
        weights[token] = count * idf.get(token, 0.0)

    return weights


def calculate_predefined_word_score(
    matched_query_words: List[str], chunk_text: str, idf: Dict[str, float]
) -> float:
    """
    Calculate TF-IDF-based score for a chunk
    Score = Σ (TF_in_chunk(word) × TF-IDF_query_weight(word)) / chunk_length
    """
    chunk_lower = chunk_text.lower()

    # Get TF-IDF-like query weights
    tfidf_weights = calculate_tfidf_weights(matched_query_words, idf)

    total_score = 0.0
    for word in matched_query_words:
        word_lower = word.lower()
        if word_lower in chunk_lower:
            # Calculate TF (term frequency in chunk)
            tf = chunk_lower.count(word_lower)

            # Get query TF-IDF weight
            query_weight = tfidf_weights.get(word_lower, 0.0)

            # Score: TF_in_chunk × TF-IDF_query_weight
            word_score = tf * query_weight
            total_score += word_score

    # Length normalization to reduce bias toward longer chunks
    chunk_tokens = tokenize_korean(chunk_text)
    length_norm = max(len(chunk_tokens), 1)
    return total_score / length_norm


# search top-3 chunks with TF-IDF-based scoring
def search_top3_weighted(
    matched_query_words: List[str],
    chunks: List[Dict[str, Any]],
    idf: Dict[str, float],
    predefined_words_list: List[str],
) -> List[tuple]:
    """
    Rank chunks using TF-IDF-like algorithm
    - Query TF-IDF weights computed once for query words
    - Chunk scores based on TF_in_chunk × query_weight
    - No cosine similarity or vector operations
    """
    scores = []
    for chunk in chunks:
        score = calculate_predefined_word_score(
            matched_query_words, chunk.get("text", ""), idf
        )
        scores.append((chunk["id"], score, chunk.get("page"), chunk.get("text")[:100]))

    # Filter out chunks with 0 score
    filtered_scores = [s for s in scores if s[1] > 0.0]

    if not filtered_scores:
        return []

    return sorted(filtered_scores, key=lambda x: -x[1])[:3]


# create TF-IDF bag of words vector (IDF score if word in query, 0 otherwise)
def tfidf_bag_of_words(
    query: str, words: List[str], vocab: Dict[str, int], idf: Dict[str, float]
) -> List[float]:
    query_lower = query.lower()
    result = []

    for word in words:
        word_lower = word.lower()

        # Check if word is a substring of query (handles cases like "포도막염" in "포도막염에")
        if word_lower in query_lower:
            # Split predefined word into tokens
            word_tokens = word_lower.split()

            # Check if all tokens are in vocab
            all_tokens_in_vocab = all(token in vocab for token in word_tokens)

            if all_tokens_in_vocab:
                # Use average IDF score of all tokens
                idf_scores = [idf.get(token, 0.0) for token in word_tokens]
                avg_idf = sum(idf_scores) / len(idf_scores)
                result.append(avg_idf)
            else:
                result.append(0.0)
        else:
            result.append(0.0)
    return result


def match_predefined_words(query: str, words: List[str]) -> List[str]:
    """
    Match predefined words in query using substring matching
    Handles space variations (e.g., "심장 사상충" matches "심장사상충")
    """
    query_lower = query.lower()
    query_no_space = query_lower.replace(" ", "")  # Remove all spaces

    matched = []
    for word in words:
        word_lower = word.lower()
        word_no_space = word_lower.replace(" ", "")

        # Check both original and space-removed versions
        if word_lower in query_lower or word_no_space in query_no_space:
            matched.append(word)

    return matched


if __name__ == "__main__":
    parser = argparse.ArgumentParser(description="Sparse TF-IDF/SPLADE scorer")
    parser.add_argument(
        "query",
        nargs="*",
        help="Query text (if omitted, will read QUERY env or interactive input)",
    )
    parser.add_argument(
        "--source-doc",
        default=os.environ.get(
            "SOURCE_DOC", "meritz_2_petpermint_dog_family_52_231.pdf"
        ),
        help="document_parser source document file name (e.g., meritz_2_petpermint_dog_family_52_231.pdf)",
    )
    parser.add_argument(
        "--use-tc-chunk-fallback",
        action="store_true",
        help="Fallback to tc_chunk when document_parser chunks are unavailable",
    )
    args = parser.parse_args()

    # predefined bag of words
    predefined_words = [
        "슬관절탈구",
        "고관절탈구",
        "슬관절형성부전",
        "고관절형성부전",
        "대퇴 골두 허혈성 괴사",
        "선천적 질병",
        "유전적 질병",
        "파보 바이러스 감염",
        "디스템퍼 바이러스 감염",
        "파라인플루엔자 감염",
        "전염성 간염",
        "아데노 바이러스 2형 감염",
        "광견병",
        "코로나 바이러스 감염",
        "렙토스피라 감염",
        "필라리아 감염",
        "심장사상충 감염",
        "인플루엔자 감염",
        "상해",
        "질병",
        "예방접종",
        "치료",
        "검사",
        "투약",
        "예방접종",
        "정기검진",
        "예방검사",
        "임신",
        "출산",
        "제왕절개",
        "인공유산",
        "증상 치료",
        "중성화 수술",
        "불임 수술",
        "피임 수술",
        "미용 시술",
        "귀 성형",
        "꼬리 성형",
        "성대 제거",
        "미용성형 수술",
        "손톱 절제",
        "며느리발톱 제거",
        "잔존유치",
        "잠복고환",
        "제대허니아",
        "배꼽부위 탈장",
        "항문낭 제거",
        "외과수술",
        "점안",
        "귀청소",
        "첩모난생",
        "속눈썹 질환",
        "눈물샘 질환",
        "식이요법",
        "의약품 처방",
        "건강보조식품",
        "한방약",
        "한의학 치료",
        "침술",
        "인도의학",
        "허브요법",
        "아로마테라피",
        "대체의료",
        "재활치료",
        "목욕",
        "약욕",
        "처방샴푸",
        "기생충 제거",
        "벼룩 감염",
        "진드기 감염",
        "모낭충 감염",
        "기생충 질환",
        "안락사",
        "해부검사",
    ]

    # load corpus chunks from document_parser outputs (preferred)
    chunks = load_chunks_from_document_parser(args.source_doc)
    if not chunks:
        chunks = load_chunks_from_terms_pdf(args.source_doc)
        if chunks:
            print("[INFO] Using PDF direct tokenization")

    if not chunks and args.use_tc_chunk_fallback:
        try:
            from tc_chunk import chunks as tc_chunks

            chunks = tc_chunks
            print("[INFO] Using tc_chunk fallback corpus")
        except Exception:
            chunks = []

    print(f"Source document: {args.source_doc}")
    print(f"Loaded chunks from source: {len(chunks)}")

    # build model for IDF calculation (vocab/idf needed for SPLADE scoring)
    if chunks:
        _, vocab, idf = build_tfidf(chunks, predefined_words)
    else:
        vocab = {word.lower(): i for i, word in enumerate(predefined_words)}
        idf = {word.lower(): 1.0 for word in predefined_words}

    # save vocab per document under terms/<source_doc_stem>/vocab.json
    source_doc_stem = Path(args.source_doc).stem
    vocab_dir = get_terms_dir() / source_doc_stem
    save_vocab_json(vocab_dir, vocab, idf, predefined_words)

    print(f"Vocab size: {len(vocab)}")
    print(f"Predefined words count: {len(predefined_words)}")

    # Option to view all vocab tokens
    view_vocab = input("\nView all vocab tokens? (y/n): ").strip().lower()
    if view_vocab == "y":
        print("\n" + "=" * 100)
        print("[Vocab Tokens]")
        print("=" * 100)
        vocab_sorted = sorted(vocab.items(), key=lambda x: x[1])
        for word, idx in vocab_sorted:
            is_predefined = (
                "✓" if word in [w.lower() for w in predefined_words] else " "
            )
            print(f"[{is_predefined}] {idx:>4}: {word}")
        print("=" * 100)

    print()

    # user input: prefer command-line arg, then env QUERY, then interactive input
    if args.query:
        query_input = " ".join(args.query).strip()
    else:
        query_input = os.environ.get("QUERY")
        if query_input:
            query_input = query_input.strip()
        else:
            query_input = input("Enter query: ").strip()

    # Tokenize query using Korean tokenizer
    print(f"Query: '{query_input}'")
    query_tokens = tokenize_korean(query_input)
    print(f"Query tokens (from tokenizer): {query_tokens}")
    print(f"  Total tokens: {len(query_tokens)}")

    # Find matched tokens in vocab
    matched_vocab_tokens = [token for token in query_tokens if token in vocab]
    not_in_vocab_tokens = [token for token in query_tokens if token not in vocab]

    print(
        f"\nMatched vocab tokens: {matched_vocab_tokens} ({len(matched_vocab_tokens)}개)"
    )
    if not_in_vocab_tokens:
        print(
            f"NOT in vocab tokens: {not_in_vocab_tokens} ({len(not_in_vocab_tokens)}개) ← vocab에 없음!"
        )

    # Find matched predefined words in query
    matched_predefined_words = match_predefined_words(query_input, predefined_words)
    print(
        f"Matched predefined words: {matched_predefined_words} ({len(matched_predefined_words)}개)"
    )
    print()

    # Calculate TF-IDF scores for matched vocab tokens
    if matched_vocab_tokens:
        tfidf_weights_vocab = calculate_tfidf_weights(matched_vocab_tokens, idf)

        print("[TF-IDF Scores - Vocab Tokens]")
        total_score_vocab = 0.0
        for token in matched_vocab_tokens:
            weight = tfidf_weights_vocab.get(token.lower(), 0.0)
            total_score_vocab += weight
            print(f"  - {token}: {weight:.4f}")

        print(f"\n[Vocab Token Total Score]: {total_score_vocab:.4f}")
    else:
        print("No matched vocab tokens found.")

    print()

    # Calculate TF-IDF scores for matched predefined words
    if matched_predefined_words:
        tfidf_weights_predefined = calculate_tfidf_weights(
            matched_predefined_words, idf
        )

        print("[TF-IDF Scores - Predefined Words]")
        total_score_predefined = 0.0
        for word in matched_predefined_words:
            weight = tfidf_weights_predefined.get(word.lower(), 0.0)
            total_score_predefined += weight
            print(f"  - {word}: {weight:.4f}")

        print(f"\n[Predefined Word Total Score]: {total_score_predefined:.4f}")
    else:
        print("No matched predefined words found.")
