from langchain.chat_models import init_chat_model

from rich import print as rprint

from app.agents.rag_agent.state.rag_state import RagState, GenerateQueryTextOutput
from app.agents.vet_agent.state import VetAgentState


def _is_meaningful_value(value: str) -> bool:
    meaningless_tokens = {
        "ì—†ìŒ",
        "í•´ë‹¹ ì—†ìŒ",
        "ì´ë ¥ ì—†ìŒ",
        "íŠ¹ì´ì‚¬í•­ ì—†ìŒ",
        "ì •ìƒ",
        "ì—†ë‹¤",
        "ë¬´",
        "none",
        "n/a",
    }
    normalized = value.strip().lower()
    return normalized not in meaningless_tokens


def generate_user_query(state: VetAgentState) -> RagState:
    rprint("ğŸ’Šê±´ê°• ìƒíƒœ:", state.health_condition)
    rprint("ğŸ’Šì§ˆë³‘:", state.diseases)

    if not state or (not state.health_condition and not state.diseases):
        raise ValueError("invalid VetAgentState !")

    frequent_illness_area_raw = (
        (state.health_condition.frequent_illness_area or "").strip()
        if state.health_condition
        else ""
    )
    frequent_illness_area = (
        frequent_illness_area_raw
        if frequent_illness_area_raw and _is_meaningful_value(frequent_illness_area_raw)
        else ""
    )
    disease_surgery_history_raw = (
        (state.health_condition.disease_surgery_history or "").strip()
        if state.health_condition
        else ""
    )
    disease_surgery_history = (
        disease_surgery_history_raw
        if disease_surgery_history_raw
        and _is_meaningful_value(disease_surgery_history_raw)
        else ""
    )
    disease_names = [
        (d.name or "").strip()
        for d in (state.diseases or [])
        if d and (d.name or "").strip() and _is_meaningful_value((d.name or "").strip())
    ]

    system_prompt = """
ì—­í• :
ë„ˆëŠ” ë°˜ë ¤ë™ë¬¼ ë³´í—˜ ì „ë¬¸ ë³´í—˜ ì„¤ê³„ì‚¬ ì¶œì‹  CTOë‹¤.

ëª©í‘œ:
ë³´í—˜ ì•½ê´€ ì²­í¬ë¥¼ dense retrievalë¡œ ì˜ ì°¾ì„ ìˆ˜ ìˆëŠ” ê³ ë°€ë„ query_textë¥¼ ìƒì„±í•œë‹¤.

ì‘ì„± ì›ì¹™:
1) ì…ë ¥ìœ¼ë¡œ ì œê³µëœ ê±´ê°• ìƒíƒœ ì •ë³´ë§Œ ì‚¬ìš©í•œë‹¤.
2) query_textëŠ” ì•½ê´€ ë³¸ë¬¸ì— ìì£¼ ë“±ì¥í•˜ëŠ” ìš©ì–´ ì¤‘ì‹¬ìœ¼ë¡œ êµ¬ì„±í•œë‹¤.
3) ê²€ìƒ‰ ë…¸ì´ì¦ˆë¥¼ ì¤„ì´ê¸° ìœ„í•´ ê°ì„±ì  í‘œí˜„/ì§ˆë¬¸í˜•/ë¶ˆí•„ìš”í•œ ìˆ˜ì‹ì–´ë¥¼ í”¼í•˜ê³ , í•µì‹¬ í‚¤ì›Œë“œ ë°€ë„ë¥¼ ë†’ì¸ë‹¤.
4) ì¶œë ¥ì€ í•œêµ­ì–´ ë‹¨ì¼ ë¬¸ì¥ 1ê°œë§Œ ìƒì„±í•œë‹¤.
"""

    reference_lines: list[str] = []
    if frequent_illness_area:
        reference_lines.append(f"- ìì£¼ ì•„í”ˆ ë¶€ìœ„: {frequent_illness_area}")
    if disease_surgery_history:
        reference_lines.append(f"- ìˆ˜ìˆ  ì´ë ¥: {disease_surgery_history}")
    if disease_names:
        reference_lines.append(f"- ì˜ˆìƒ ì§ˆë³‘: {', '.join(disease_names)}")

    reference_block = (
        "\n".join(reference_lines) if reference_lines else "- ì¶”ê°€ ê±´ê°• ì •ë³´ ì—†ìŒ"
    )

    user_prompt = f"""
ì•½ê´€ ê²€ìƒ‰ìš© query_text 1ê°œë¥¼ ìƒì„±í•´ì¤˜.

[ì°¸ê³  ì •ë³´]
{reference_block}

[ìƒì„± ê·œì¹™]
1) ì°¸ê³  ì •ë³´ë¥¼ ë°˜ë“œì‹œ í¬í•¨í•˜ê³ , í•´ë‹¹ ì •ë³´ì™€ ì§ì ‘ì ìœ¼ë¡œ ì—°ê´€ëœ ì•½ê´€ ìš©ì–´ë¥¼ í•¨ê»˜ ë„£ëŠ”ë‹¤.
2) íŠ¹íˆ 'ë³´ì¥(coverage)'ê³¼ ê´€ë ¨ëœ ìš©ì–´ë¥¼ ìì—°ìŠ¤ëŸ½ê²Œ ì¶”ê°€í•œë‹¤.
3) "ì—†ìŒ", "í•´ë‹¹ ì—†ìŒ", "ì´ë ¥ ì—†ìŒ", "íŠ¹ì´ì‚¬í•­ ì—†ìŒ" ê°™ì€ ì˜ë¯¸ ì—†ëŠ” ê°’ì€ ì‚¬ìš©í•˜ì§€ ì•ŠëŠ”ë‹¤.
4) ì¶œë ¥ì€ ì§ˆë¬¸í˜•ì´ ì•„ë‹Œ, ê²€ìƒ‰ ì¸í…íŠ¸ê°€ ë¶„ëª…í•œ ë‹¨ì¼ í•œêµ­ì–´ ë¬¸ì¥ìœ¼ë¡œ ì‘ì„±í•œë‹¤.
""".strip()
    # rprint("ğŸ‘‰ğŸ»ìƒì„±ëœ user_prompt:", user_prompt)

    MODEL = "solar-pro2"
    llm = init_chat_model(model=MODEL, temperature=0.0)
    structured_llm = llm.with_structured_output(GenerateQueryTextOutput)
    llm_response: GenerateQueryTextOutput = structured_llm.invoke(
        [
            {"role": "system", "content": system_prompt},
            {"role": "user", "content": user_prompt},
        ]
    )
    rprint("â“ìƒì„±ëœ query text:", llm_response.query_text)

    return {
        # "user_query": llm_response.query_text,
        "query_texts": [llm_response.query_text],
    }
